{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722bff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from tabulate import tabulate\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fab47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_json='../../data/processed/final_annotations_without_occluded.json'\n",
    "image_directory = '../../data/images'\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "random_seed = 42\n",
    "\n",
    "with open(final_output_json, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_filenames = list(annotations['images'].keys())[:500]\n",
    "\n",
    "random.seed(random_seed)\n",
    "random.shuffle(image_filenames)\n",
    "\n",
    "num_test = int(len(image_filenames) * test_ratio)\n",
    "test_images = image_filenames[:num_test]\n",
    "train_images = image_filenames[num_test:]\n",
    "num_valid = int(len(train_images) * valid_ratio)\n",
    "valid_images = train_images[:num_valid]\n",
    "\n",
    "train_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in train_images}\n",
    "}\n",
    "\n",
    "valid_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in valid_images}\n",
    "}\n",
    "\n",
    "test_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in test_images}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f76445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikePartsDetectionDataset(Dataset):\n",
    "    def __init__(self, annotations_dict, image_dir, transform=None, augment=True, target_size=(640, 640)):\n",
    "        self.all_parts = annotations_dict['all_parts']\n",
    "        self.part_to_idx = {part: idx + 1 for idx, part in enumerate(self.all_parts)}\n",
    "        self.idx_to_part = {idx + 1: part for idx, part in enumerate(self.all_parts)}\n",
    "        self.image_data = annotations_dict['images']\n",
    "        self.image_filenames = list(self.image_data.keys())\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames) * (2 if self.augment else 1)\n",
    "\n",
    "    def apply_augmentation(self, image, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            w = image.width\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_brightness(image, brightness_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_contrast(image, contrast_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.adjust_saturation(image, saturation_factor=random.uniform(0.7, 1.3))\n",
    "\n",
    "        return image, boxes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.image_filenames)\n",
    "        do_augment = self.augment and (idx >= len(self.image_filenames))\n",
    "\n",
    "        img_filename = self.image_filenames[real_idx]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_width, orig_height = image.size\n",
    "\n",
    "        annotation = self.image_data[img_filename]\n",
    "        available_parts_info = annotation['available_parts']\n",
    "        missing_parts_names = annotation.get('missing_parts', [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for part_info in available_parts_info:\n",
    "            part_name = part_info['part_name']\n",
    "            bbox = part_info['absolute_bounding_box']\n",
    "            xmin = bbox['left']\n",
    "            ymin = bbox['top']\n",
    "            xmax = xmin + bbox['width']\n",
    "            ymax = ymin + bbox['height']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.part_to_idx[part_name])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        if do_augment:\n",
    "            image, boxes = self.apply_augmentation(image, boxes)\n",
    "\n",
    "        image = transforms.functional.resize(image, self.target_size)\n",
    "        new_width, new_height = self.target_size\n",
    "        scale_x = new_width / orig_width\n",
    "        scale_y = new_height / orig_height\n",
    "        boxes[:, [0, 2]] *= scale_x\n",
    "        boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "\n",
    "        missing_labels = torch.tensor(\n",
    "            [self.part_to_idx[part] for part in missing_parts_names],\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'missing_labels': missing_labels,\n",
    "            'image_id': torch.tensor([real_idx])\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=train_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "valid_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=valid_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=test_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7098798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 200/200 [17:27<00:00,  5.24s/batch, loss=1.0818, time (s)=5.116, GPU Mem (MB)=0, CPU Mem (MB)=8702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    1    |\n",
      "|          Final Loss           | 1.0818  |\n",
      "|   Average Batch Time (sec)    | 5.0300  |\n",
      "| Average GPU Memory Usage (MB) |  0.00   |\n",
      "| Average CPU Memory Usage (MB) | 8706.79 |\n",
      "+-------------------------------+---------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights='DEFAULT')\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "num_classes = len(train_dataset.all_parts) + 1\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    batch_times = []\n",
    "    gpu_memories = []\n",
    "    cpu_memories = []\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as tepoch:\n",
    "        for images, targets in tepoch:\n",
    "            start_time = time.time()\n",
    "\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            end_time = time.time()\n",
    "            inference_time = end_time - start_time\n",
    "            batch_times.append(inference_time)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_mem_used = mem_info.used / (1024 ** 2)\n",
    "                gpu_memories.append(gpu_mem_used)\n",
    "            else:\n",
    "                gpu_mem_used = 0\n",
    "\n",
    "            cpu_mem_used = psutil.virtual_memory().used / (1024 ** 2)\n",
    "            cpu_memories.append(cpu_mem_used)\n",
    "\n",
    "            tepoch.set_postfix({\n",
    "                \"loss\": f\"{losses.item():.4f}\",\n",
    "                \"time (s)\": f\"{inference_time:.3f}\",\n",
    "                \"GPU Mem (MB)\": f\"{gpu_mem_used:.0f}\",\n",
    "                \"CPU Mem (MB)\": f\"{cpu_mem_used:.0f}\"\n",
    "            })\n",
    "\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    avg_time = sum(batch_times) / len(batch_times)\n",
    "    avg_gpu_mem = sum(gpu_memories) / len(gpu_memories) if gpu_memories else 0\n",
    "    avg_cpu_mem = sum(cpu_memories) / len(cpu_memories)\n",
    "\n",
    "    table = [\n",
    "        [\"Epoch\", epoch + 1],\n",
    "        [\"Final Loss\", f\"{losses.item():.4f}\"],\n",
    "        [\"Average Batch Time (sec)\", f\"{avg_time:.4f}\"],\n",
    "        [\"Average GPU Memory Usage (MB)\", f\"{avg_gpu_mem:.2f}\"],\n",
    "        [\"Average CPU Memory Usage (MB)\", f\"{avg_cpu_mem:.2f}\"],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlShutdown()\n",
    "\n",
    "# torch.save(model.state_dict(), f\"../../models/faster_rcnn/fasterrcnn_{num_epochs}_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fbcd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:26<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 0, 'predicted_missing_parts': {22}, 'true_missing_parts': {1, 9, 16, 17, 18, 21, 22}}, {'image_id': 1, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {1, 9, 13, 14, 19}}, {'image_id': 2, 'predicted_missing_parts': {6, 9, 11, 16, 19}, 'true_missing_parts': {2, 6, 7, 9, 11, 13, 16, 17, 19, 21, 22}}, {'image_id': 3, 'predicted_missing_parts': {2, 18, 5, 22}, 'true_missing_parts': {2, 5, 7, 8, 9, 11, 16, 17, 18, 19, 21, 22}}, {'image_id': 4, 'predicted_missing_parts': {18, 5}, 'true_missing_parts': {2, 5, 9, 11, 17, 18, 19, 21}}, {'image_id': 5, 'predicted_missing_parts': {18}, 'true_missing_parts': {2, 7, 11, 12, 16, 19, 22}}, {'image_id': 6, 'predicted_missing_parts': {18, 19}, 'true_missing_parts': {1, 9, 11, 13, 16, 17, 18, 22}}, {'image_id': 7, 'predicted_missing_parts': {18, 19}, 'true_missing_parts': {1, 8, 17, 18, 19, 21, 22}}, {'image_id': 8, 'predicted_missing_parts': {18}, 'true_missing_parts': {1, 2, 7, 9, 11, 13, 18, 19}}, {'image_id': 9, 'predicted_missing_parts': {16, 9, 18, 5}, 'true_missing_parts': {2, 5, 6, 7, 9, 11, 13, 16, 19, 21}}, {'image_id': 10, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {5, 9, 13, 17, 18, 19, 22}}, {'image_id': 11, 'predicted_missing_parts': {16, 19}, 'true_missing_parts': {4, 5, 6, 7, 9, 11, 12, 16, 17, 18, 19, 20, 21, 22}}, {'image_id': 12, 'predicted_missing_parts': set(), 'true_missing_parts': {1, 18, 13}}, {'image_id': 13, 'predicted_missing_parts': {18, 19, 22}, 'true_missing_parts': {5, 9, 12, 17, 19, 21, 22}}, {'image_id': 14, 'predicted_missing_parts': {9, 18}, 'true_missing_parts': {1, 9, 11, 12, 13, 16, 18}}, {'image_id': 15, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {9, 1, 22, 7}}, {'image_id': 16, 'predicted_missing_parts': {19, 2, 18}, 'true_missing_parts': {1, 2, 5, 7, 9, 11, 12, 14, 17, 18, 19, 20, 21, 22}}, {'image_id': 17, 'predicted_missing_parts': {18}, 'true_missing_parts': {9, 14, 16, 18, 19, 21}}, {'image_id': 18, 'predicted_missing_parts': {18}, 'true_missing_parts': {1, 9, 11, 13, 18, 19}}, {'image_id': 19, 'predicted_missing_parts': {18}, 'true_missing_parts': {1, 5, 7, 9, 13, 19}}, {'image_id': 20, 'predicted_missing_parts': {18, 19, 5, 22}, 'true_missing_parts': {1, 5, 9, 13, 18, 19, 22}}, {'image_id': 21, 'predicted_missing_parts': {18}, 'true_missing_parts': {16, 9, 13}}, {'image_id': 22, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {9, 11, 21}}, {'image_id': 23, 'predicted_missing_parts': {9, 18, 22}, 'true_missing_parts': {1, 2, 6, 7, 9, 11, 13, 16, 17, 22}}, {'image_id': 24, 'predicted_missing_parts': set(), 'true_missing_parts': {1, 2, 5, 7, 9, 13, 17, 18}}, {'image_id': 25, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {1, 8, 9, 16, 17, 18, 21}}, {'image_id': 26, 'predicted_missing_parts': {9, 11, 16, 18, 19, 22}, 'true_missing_parts': {1, 2, 4, 5, 6, 7, 9, 11, 16, 18, 19, 21, 22}}, {'image_id': 27, 'predicted_missing_parts': {18}, 'true_missing_parts': {16, 9, 19, 13}}, {'image_id': 28, 'predicted_missing_parts': {18, 5}, 'true_missing_parts': {1, 13, 19, 5}}, {'image_id': 29, 'predicted_missing_parts': {2, 18}, 'true_missing_parts': {1, 2, 7, 9, 12, 14, 16, 18, 22}}, {'image_id': 30, 'predicted_missing_parts': {9, 18, 5}, 'true_missing_parts': {1, 9, 13, 16, 22}}, {'image_id': 31, 'predicted_missing_parts': set(), 'true_missing_parts': {2, 5, 7, 9, 11, 13, 14, 17, 18}}, {'image_id': 32, 'predicted_missing_parts': {18, 22}, 'true_missing_parts': {1, 2, 5, 13, 18, 19}}, {'image_id': 33, 'predicted_missing_parts': {2, 18}, 'true_missing_parts': {1, 2, 5, 7, 8, 9, 13, 16, 18, 19}}, {'image_id': 34, 'predicted_missing_parts': {16, 18, 22}, 'true_missing_parts': {6, 9, 12, 13, 14, 16, 22}}, {'image_id': 35, 'predicted_missing_parts': {18, 5, 22}, 'true_missing_parts': {2, 5, 7, 9, 13, 16, 17, 18, 22}}, {'image_id': 36, 'predicted_missing_parts': {16, 5, 22}, 'true_missing_parts': {1, 2, 3, 5, 7, 9, 11, 13, 15, 16, 22}}, {'image_id': 37, 'predicted_missing_parts': {9, 16, 18, 19, 22}, 'true_missing_parts': {1, 6, 8, 9, 16, 18, 19, 21, 22}}, {'image_id': 38, 'predicted_missing_parts': {19, 2, 18}, 'true_missing_parts': {1, 2, 7, 17, 18, 19, 21, 22}}, {'image_id': 39, 'predicted_missing_parts': {8, 18, 22}, 'true_missing_parts': {5, 8, 9, 11, 13, 18, 19, 22}}]\n",
      "╒═════════════════╤════════════╤═════════════╤══════════╤════════════╕\n",
      "│ Part            │   Accuracy │   Precision │   Recall │   F1 Score │\n",
      "╞═════════════════╪════════════╪═════════════╪══════════╪════════════╡\n",
      "│ back_pedal      │      0.4   │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_hand_break │      0.7   │      1      │   0.2941 │     0.4545 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_pedal     │      0.975 │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_mudguard  │      0.95  │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_light     │      0.7   │      0.875  │   0.3889 │     0.5385 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_mudguard   │      0.85  │      1      │   0.1429 │     0.25   │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_handbreak │      0.55  │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ saddle          │      0.875 │      1      │   0.1667 │     0.2857 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ dress_guard     │      0.325 │      1      │   0.2059 │     0.3415 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_wheel      │      1     │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ lock            │      0.625 │      1      │   0.1176 │     0.2105 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_handle     │      0.825 │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ chain           │      0.4   │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_handle    │      0.85  │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ front_wheel     │      0.975 │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_reflector  │      0.65  │      1      │   0.3333 │     0.5    │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ kickstand       │      0.6   │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ bell            │      0.55  │      0.6061 │   0.8    │     0.6897 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ back_light      │      0.6   │      0.9    │   0.375  │     0.5294 │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ steer           │      0.95  │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ gear_case       │      0.625 │      0      │   0      │     0      │\n",
      "├─────────────────┼────────────┼─────────────┼──────────┼────────────┤\n",
      "│ dynamo          │      0.675 │      0.7647 │   0.5909 │     0.6667 │\n",
      "╘═════════════════╧════════════╧═════════════╧══════════╧════════════╛\n",
      "\n",
      "Overall Metrics:\n",
      "╒════════════╤═════════════╤══════════╤════════════╕\n",
      "│   Accuracy │   Precision │   Recall │   F1 Score │\n",
      "╞════════════╪═════════════╪══════════╪════════════╡\n",
      "│     0.7114 │      0.7912 │   0.2345 │     0.3618 │\n",
      "╘════════════╧═════════════╧══════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, part_to_idx, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_parts_set = set(part_to_idx.values())\n",
    "    results_per_image = []\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            pred_parts = set(predictions[i]['labels'].cpu().numpy().tolist())\n",
    "            true_missing_parts = set(targets[i]['missing_labels'].cpu().numpy().tolist())\n",
    "            image_id = targets[i]['image_id'].item()\n",
    "\n",
    "            predicted_missing_parts = all_parts_set - pred_parts\n",
    "\n",
    "            results_per_image.append({\n",
    "                'image_id': image_id,\n",
    "                'predicted_missing_parts': predicted_missing_parts,\n",
    "                'true_missing_parts': true_missing_parts\n",
    "            })\n",
    "\n",
    "    return results_per_image\n",
    "\n",
    "\n",
    "def part_level_evaluation(results_per_image, part_to_idx, idx_to_part):\n",
    "    part_indices = list(part_to_idx.values())\n",
    "\n",
    "    part_true = {part: [] for part in part_indices}\n",
    "    part_pred = {part: [] for part in part_indices}\n",
    "\n",
    "    for result in results_per_image:\n",
    "        true_missing = result['true_missing_parts']\n",
    "        predicted_missing = result['predicted_missing_parts']\n",
    "\n",
    "        for part in part_indices:\n",
    "            part_true[part].append(1 if part in true_missing else 0)\n",
    "            part_pred[part].append(1 if part in predicted_missing else 0)\n",
    "\n",
    "    accuracy = {}\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "\n",
    "    all_true_flat = []\n",
    "    all_pred_flat = []\n",
    "\n",
    "    table_rows = []\n",
    "\n",
    "    for part in part_indices:\n",
    "        y_true = part_true[part]\n",
    "        y_pred = part_pred[part]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1s = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        accuracy[part] = acc\n",
    "        precision[part] = prec\n",
    "        recall[part] = rec\n",
    "        f1[part] = f1s\n",
    "\n",
    "        all_true_flat.extend(y_true)\n",
    "        all_pred_flat.extend(y_pred)\n",
    "\n",
    "        table_rows.append([\n",
    "            idx_to_part[part], f\"{acc:.4f}\", f\"{prec:.4f}\", f\"{rec:.4f}\", f\"{f1s:.4f}\"\n",
    "        ])\n",
    "\n",
    "    print(tabulate(table_rows, headers=[\"Part\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    overall_accuracy = accuracy_score(all_true_flat, all_pred_flat)\n",
    "    overall_precision = precision_score(all_true_flat, all_pred_flat, zero_division=0)\n",
    "    overall_recall = recall_score(all_true_flat, all_pred_flat, zero_division=0)\n",
    "    overall_f1 = f1_score(all_true_flat, all_pred_flat, zero_division=0)\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(tabulate([[\n",
    "        f\"{overall_accuracy:.4f}\", f\"{overall_precision:.4f}\",\n",
    "        f\"{overall_recall:.4f}\", f\"{overall_f1:.4f}\"\n",
    "    ]], headers=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    return accuracy, precision, recall, f1, overall_accuracy, overall_precision, overall_recall, overall_f1\n",
    "\n",
    "\n",
    "results_per_image = evaluate_model(model, valid_loader, train_dataset.part_to_idx, device)\n",
    "# results_per_image = evaluate_model(model, test_loader, train_dataset.part_to_idx, device)\n",
    "\n",
    "print(results_per_image)\n",
    "\n",
    "accuracy, precision, recall, f1, overall_accuracy, overall_precision, overall_recall, overall_f1 = part_level_evaluation(\n",
    "    results_per_image, train_dataset.part_to_idx, train_dataset.idx_to_part\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
