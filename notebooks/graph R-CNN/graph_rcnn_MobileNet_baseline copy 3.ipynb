{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "722bff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data as GraphData\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import dense_to_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95333ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % (2**32)\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_json='../../data/processed/final_annotations_without_occluded.json'\n",
    "image_directory = '../../data/images'\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "random_seed = 42\n",
    "\n",
    "with open(final_output_json, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_filenames = list(annotations['images'].keys())[:100]\n",
    "\n",
    "random.seed(random_seed)\n",
    "random.shuffle(image_filenames)\n",
    "\n",
    "num_test = int(len(image_filenames) * test_ratio)\n",
    "test_images = image_filenames[:num_test]\n",
    "train_images = image_filenames[num_test:]\n",
    "num_valid = int(len(train_images) * valid_ratio)\n",
    "valid_images = train_images[:num_valid]\n",
    "\n",
    "train_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in train_images}\n",
    "}\n",
    "\n",
    "valid_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in valid_images}\n",
    "}\n",
    "\n",
    "test_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in test_images}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76f76445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikePartsDetectionDataset(Dataset):\n",
    "    def __init__(self, annotations_dict, image_dir, transform=None, augment=True, target_size=(640, 640)):\n",
    "        self.all_parts = annotations_dict['all_parts']\n",
    "        self.part_to_idx = {part: idx + 1 for idx, part in enumerate(self.all_parts)}\n",
    "        self.idx_to_part = {idx + 1: part for idx, part in enumerate(self.all_parts)}\n",
    "        self.image_data = annotations_dict['images']\n",
    "        self.image_filenames = list(self.image_data.keys())\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames) * (2 if self.augment else 1)\n",
    "\n",
    "    def apply_augmentation(self, image, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            w = image.width\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_brightness(image, brightness_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_contrast(image, contrast_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.adjust_saturation(image, saturation_factor=random.uniform(0.7, 1.3))\n",
    "\n",
    "        return image, boxes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.image_filenames)\n",
    "        do_augment = self.augment and (idx >= len(self.image_filenames))\n",
    "\n",
    "        img_filename = self.image_filenames[real_idx]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_width, orig_height = image.size\n",
    "\n",
    "        annotation = self.image_data[img_filename]\n",
    "        available_parts_info = annotation['available_parts']\n",
    "        missing_parts_names = annotation.get('missing_parts', [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for part_info in available_parts_info:\n",
    "            part_name = part_info['part_name']\n",
    "            bbox = part_info['absolute_bounding_box']\n",
    "            xmin = bbox['left']\n",
    "            ymin = bbox['top']\n",
    "            xmax = xmin + bbox['width']\n",
    "            ymax = ymin + bbox['height']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.part_to_idx[part_name])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        if do_augment:\n",
    "            image, boxes = self.apply_augmentation(image, boxes)\n",
    "\n",
    "        image = transforms.functional.resize(image, self.target_size)\n",
    "        new_width, new_height = self.target_size\n",
    "        scale_x = new_width / orig_width\n",
    "        scale_y = new_height / orig_height\n",
    "        boxes[:, [0, 2]] *= scale_x\n",
    "        boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "\n",
    "        missing_labels = torch.tensor(\n",
    "            [self.part_to_idx[part] for part in missing_parts_names],\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'missing_labels': missing_labels,\n",
    "            'image_id': torch.tensor([real_idx])\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0904d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=train_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "valid_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=valid_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=test_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef969e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, part_to_idx, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_parts_set = set(part_to_idx.values())\n",
    "    results_per_image = []\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            pred_parts = set(predictions[i]['labels'].cpu().numpy().tolist())\n",
    "            true_missing_parts = set(targets[i]['missing_labels'].cpu().numpy().tolist())\n",
    "            image_id = targets[i]['image_id'].item()\n",
    "\n",
    "            predicted_missing_parts = all_parts_set - pred_parts\n",
    "\n",
    "            results_per_image.append({\n",
    "                'image_id': image_id,\n",
    "                'predicted_missing_parts': predicted_missing_parts,\n",
    "                'true_missing_parts': true_missing_parts\n",
    "            })\n",
    "\n",
    "    return results_per_image\n",
    "\n",
    "\n",
    "def part_level_evaluation(results, part_to_idx, idx_to_part):\n",
    "    parts = list(part_to_idx.values())\n",
    "\n",
    "    Y_true = np.array([[1 if p in r['true_missing_parts'] else 0 for p in parts] for r in results])\n",
    "    Y_pred = np.array([[1 if p in r['predicted_missing_parts'] else 0 for p in parts] for r in results])\n",
    "\n",
    "    micro_f1 = f1_score(Y_true, Y_pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(Y_true, Y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    FN = np.logical_and(Y_true==1, Y_pred==0).sum()\n",
    "    TP = np.logical_and(Y_true==1, Y_pred==1).sum()\n",
    "    FP = np.logical_and(Y_true==0, Y_pred==1).sum()\n",
    "\n",
    "    N_images = len(results)\n",
    "    miss_rate = FN/(FN+TP) if (FN+TP)>0 else 0\n",
    "    fppi = FP/N_images\n",
    "\n",
    "    overall_acc = accuracy_score(Y_true.flatten(), Y_pred.flatten())\n",
    "    overall_prec = precision_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_rec = recall_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_f1 = f1_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    print(f\"Micro-F1: {micro_f1:.4f}, Macro-F1: {macro_f1:.4f}\")\n",
    "    print(f\"Miss Rate: {miss_rate:.4f}, FPPI: {fppi:.4f}\")\n",
    "    print(f\"Overall Acc: {overall_acc:.4f}, Precision: {overall_prec:.4f}, Recall: {overall_rec:.4f}, F1: {overall_f1:.4f}\")\n",
    "    \n",
    "    table=[]\n",
    "    for j,p in enumerate(parts):\n",
    "        acc = accuracy_score(Y_true[:,j], Y_pred[:,j])\n",
    "        prec = precision_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        rec = recall_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        f1s = f1_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        table.append([idx_to_part[p], f\"{acc:.3f}\", f\"{prec:.3f}\", f\"{rec:.3f}\", f\"{f1s:.3f}\"])\n",
    "    print(tabulate(table, headers=[\"Part\",\"Acc\",\"Prec\",\"Rec\",\"F1\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GraphRCNNWithDualPrediction(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_channels=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.detector = fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "        in_features = self.detector.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.detector.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        self.gnn_head = GNNHead(in_channels=in_features, hidden_channels=hidden_channels, out_channels=num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.gnn_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training:\n",
    "            detector_losses = self.detector(images, targets)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_list, _ = self.detector.transform(images, targets)\n",
    "                features = self.detector.backbone(image_list.tensors)\n",
    "            image_shapes = image_list.image_sizes\n",
    "            proposals = [t[\"boxes\"].detach() for t in targets]\n",
    "\n",
    "            roi_pooled = self.detector.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "            roi_features = self.detector.roi_heads.box_head(roi_pooled)\n",
    "\n",
    "            gnn_losses = 0.0\n",
    "            start = 0\n",
    "            gnn_loss_fn = nn.CrossEntropyLoss()  # define once outside loop\n",
    "\n",
    "            for i, boxes in enumerate(proposals):\n",
    "                num_nodes = boxes.shape[0]\n",
    "                if num_nodes == 0:\n",
    "                    continue\n",
    "\n",
    "                roi_feat = roi_features[start:start + num_nodes]\n",
    "                edge_index = self._build_fully_connected_edges(num_nodes).to(roi_feat.device)\n",
    "\n",
    "                gnn_logits = self.gnn_head(roi_feat, edge_index)  # [num_boxes, num_classes]\n",
    "\n",
    "                part_labels = targets[i][\"labels\"].to(roi_feat.device)  # [num_boxes]\n",
    "                gnn_loss = gnn_loss_fn(gnn_logits, part_labels)  # CrossEntropyLoss\n",
    "\n",
    "                gnn_losses += gnn_loss\n",
    "                start += num_nodes\n",
    "\n",
    "            total_loss = sum(detector_losses.values()) + gnn_losses\n",
    "\n",
    "            return {\n",
    "                **detector_losses,\n",
    "                \"loss_gnn\": gnn_losses,\n",
    "                \"loss_total\": total_loss\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            detections = self.detector(images)\n",
    "            all_outputs = []\n",
    "\n",
    "            for img_idx, det in enumerate(detections):\n",
    "                boxes = det[\"boxes\"]\n",
    "\n",
    "                if boxes.shape[0] == 0:\n",
    "                    all_outputs.append({\n",
    "                        \"boxes\": boxes,\n",
    "                        \"scores\": det[\"scores\"],\n",
    "                        \"labels\": torch.tensor([], dtype=torch.long, device=boxes.device)\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                features = self._get_roi_features(images[img_idx].unsqueeze(0), boxes)\n",
    "\n",
    "                num_nodes = features.shape[0]\n",
    "                edge_index = self._build_fully_connected_edges(num_nodes).to(features.device)\n",
    "                gnn_logits = self.gnn_head(features, edge_index)  # [num_boxes, num_classes]\n",
    "\n",
    "                # For each box, predict a single label via argmax over classes\n",
    "                per_box_labels = gnn_logits.argmax(dim=1)  # [num_boxes] -> int labels\n",
    "                unique_labels = per_box_labels.unique()\n",
    "\n",
    "                final_output = {\n",
    "                    \"boxes\": boxes,\n",
    "                    \"scores\": det[\"scores\"],\n",
    "                    \"labels\": unique_labels  # predicted part classes present in image\n",
    "                }\n",
    "                all_outputs.append(final_output)\n",
    "\n",
    "            return all_outputs\n",
    "\n",
    "\n",
    "    def _get_roi_features(self, image, boxes):\n",
    "        with torch.no_grad():\n",
    "            features = self.detector.backbone(image.tensors if hasattr(image, 'tensors') else image)\n",
    "\n",
    "        image_shapes = [img.shape[-2:] for img in image]\n",
    "        roi_pooled = self.detector.roi_heads.box_roi_pool(features, [boxes], image_shapes)\n",
    "        roi_features = self.detector.roi_heads.box_head(roi_pooled)\n",
    "        return roi_features\n",
    "\n",
    "    def _build_fully_connected_edges(self, num_nodes):\n",
    "        adj = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)\n",
    "        return dense_to_sparse(adj)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7098798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 100/100 [12:15<00:00,  7.35s/batch, loss=25.9231, time (s)=6.872, GPU Mem (MB)=0, CPU Mem (MB)=9767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------+\n",
      "|            Metric             |   Value    |\n",
      "+-------------------------------+------------+\n",
      "|             Epoch             |     1      |\n",
      "|          Final Loss           |  25.9231   |\n",
      "|   Average Batch Time (sec)    |   7.0983   |\n",
      "| Average GPU Memory Usage (MB) |    0.00    |\n",
      "| Average CPU Memory Usage (MB) |  9958.03   |\n",
      "|   Energy Consumption (kWh)    | 0.0034 kWh |\n",
      "|      CO₂ Emissions (kg)       | 0.0009 kg  |\n",
      "+-------------------------------+------------+\n",
      "\n",
      "Evaluating on validation set after Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([1, 3, 640, 640])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(tabulate(table, headers=[\u001b[33m\"\u001b[39m\u001b[33mMetric\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m], tablefmt=\u001b[33m\"\u001b[39m\u001b[33mpretty\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating on validation set after Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m results_per_image = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpart_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m parts = \u001b[38;5;28mlist\u001b[39m(train_dataset.part_to_idx.values())\n\u001b[32m     96\u001b[39m Y_true = np.array([[\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m r[\u001b[33m'\u001b[39m\u001b[33mtrue_missing_parts\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parts] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results_per_image])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, data_loader, part_to_idx, device)\u001b[39m\n\u001b[32m      9\u001b[39m targets = [{k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t.items()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[32m     15\u001b[39m     pred_parts = \u001b[38;5;28mset\u001b[39m(predictions[i][\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].cpu().numpy().tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mGraphRCNNWithDualPrediction.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Apply transform and get ROI features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_roi_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m num_nodes = features.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     87\u001b[39m edge_index = \u001b[38;5;28mself\u001b[39m._build_fully_connected_edges(num_nodes).to(features.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mGraphRCNNWithDualPrediction._get_roi_features\u001b[39m\u001b[34m(self, image, boxes)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_roi_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, boxes):\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Apply transform for consistent backbone input\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Image is a single image, so we need to pass it as a list\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     image_list, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass image as a list\u001b[39;00m\n\u001b[32m    115\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.detector.backbone(image_list.tensors)\n\u001b[32m    117\u001b[39m     image_shapes = image_list.image_sizes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/detection/transform.py:140\u001b[39m, in \u001b[36mGeneralizedRCNNTransform.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    137\u001b[39m target_index = targets[i] \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.dim() != \u001b[32m3\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m image = \u001b[38;5;28mself\u001b[39m.normalize(image)\n\u001b[32m    142\u001b[39m image, target_index = \u001b[38;5;28mself\u001b[39m.resize(image, target_index)\n",
      "\u001b[31mValueError\u001b[39m: images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([1, 3, 640, 640])"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = len(train_dataset.all_parts) + 1\n",
    "model = GraphRCNNWithDualPrediction(num_classes=num_classes, hidden_channels=512)\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "num_epochs = 1\n",
    "best_macro_f1 = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 3\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    with EmissionsTracker(log_level=\"critical\", save_to_file=False) as tracker:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        batch_times = []\n",
    "        gpu_memories = []\n",
    "        cpu_memories = []\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as tepoch:\n",
    "            for images, targets in tepoch:\n",
    "                start_time = time.time()\n",
    "\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "\n",
    "\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                end_time = time.time()\n",
    "                inference_time = end_time - start_time\n",
    "                batch_times.append(inference_time)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                    gpu_mem_used = mem_info.used / (1024 ** 2)\n",
    "                    gpu_memories.append(gpu_mem_used)\n",
    "                else:\n",
    "                    gpu_mem_used = 0\n",
    "\n",
    "                cpu_mem_used = psutil.virtual_memory().used / (1024 ** 2)\n",
    "                cpu_memories.append(cpu_mem_used)\n",
    "\n",
    "                tepoch.set_postfix({\n",
    "                    \"loss\": f\"{losses.item():.4f}\",\n",
    "                    \"time (s)\": f\"{inference_time:.3f}\",\n",
    "                    \"GPU Mem (MB)\": f\"{gpu_mem_used:.0f}\",\n",
    "                    \"CPU Mem (MB)\": f\"{cpu_mem_used:.0f}\"\n",
    "                })\n",
    "\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    energy_consumption = tracker.final_emissions_data.energy_consumed\n",
    "    co2_emissions = tracker.final_emissions\n",
    "\n",
    "    avg_time = sum(batch_times) / len(batch_times)\n",
    "    max_gpu_mem = max(gpu_memories) if gpu_memories else 0\n",
    "    max_cpu_mem = max(cpu_memories)\n",
    "\n",
    "    table = [\n",
    "        [\"Epoch\", epoch + 1],\n",
    "        [\"Final Loss\", f\"{losses.item():.4f}\"],\n",
    "        [\"Average Batch Time (sec)\", f\"{avg_time:.4f}\"],\n",
    "        [\"Average GPU Memory Usage (MB)\", f\"{max_gpu_mem:.2f}\"],\n",
    "        [\"Average CPU Memory Usage (MB)\", f\"{max_cpu_mem:.2f}\"],\n",
    "        [\"Energy Consumption (kWh)\", f\"{energy_consumption:.4f} kWh\"],\n",
    "        [\"CO₂ Emissions (kg)\", f\"{co2_emissions:.4f} kg\"],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))\n",
    "\n",
    "    print(f\"\\nEvaluating on validation set after Epoch {epoch + 1}...\")\n",
    "    results_per_image = evaluate_model(model, valid_loader, train_dataset.part_to_idx, device)\n",
    "\n",
    "    parts = list(train_dataset.part_to_idx.values())\n",
    "    Y_true = np.array([[1 if p in r['true_missing_parts'] else 0 for p in parts] for r in results_per_image])\n",
    "    Y_pred = np.array([[1 if p in r['predicted_missing_parts'] else 0 for p in parts] for r in results_per_image])\n",
    "    macro_f1 = f1_score(Y_true, Y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), f\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\")\n",
    "        print(f\"Saved new best model (macro-F1: {macro_f1:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement in macro-F1 for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbcd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57557/349410656.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\", map_location=device))\n",
      "Evaluating: 100%|██████████| 10/10 [00:35<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 0, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22}, 'true_missing_parts': {1, 9, 16, 17, 18, 21, 22}}, {'image_id': 1, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 13, 14, 19}}, {'image_id': 2, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 6, 7, 9, 11, 13, 16, 17, 19, 21, 22}}, {'image_id': 3, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 5, 7, 8, 9, 11, 16, 17, 18, 19, 21, 22}}, {'image_id': 4, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 5, 9, 11, 17, 18, 19, 21}}, {'image_id': 5, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}, 'true_missing_parts': {2, 7, 11, 12, 16, 19, 22}}, {'image_id': 6, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 11, 13, 16, 17, 18, 22}}, {'image_id': 7, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 8, 17, 18, 19, 21, 22}}, {'image_id': 8, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 7, 9, 11, 13, 18, 19}}, {'image_id': 9, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 5, 6, 7, 9, 11, 13, 16, 19, 21}}, {'image_id': 10, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {5, 9, 13, 17, 18, 19, 22}}, {'image_id': 11, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}, 'true_missing_parts': {4, 5, 6, 7, 9, 11, 12, 16, 17, 18, 19, 20, 21, 22}}, {'image_id': 12, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 18, 13}}, {'image_id': 13, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}, 'true_missing_parts': {5, 9, 12, 17, 19, 21, 22}}, {'image_id': 14, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 11, 12, 13, 16, 18}}, {'image_id': 15, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {9, 1, 22, 7}}, {'image_id': 16, 'predicted_missing_parts': {1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 7, 9, 11, 12, 14, 17, 18, 19, 20, 21, 22}}, {'image_id': 17, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {9, 14, 16, 18, 19, 21}}, {'image_id': 18, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 11, 13, 18, 19}}, {'image_id': 19, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 5, 7, 9, 13, 19}}, {'image_id': 20, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 5, 9, 13, 18, 19, 22}}, {'image_id': 21, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {16, 9, 13}}, {'image_id': 22, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {9, 11, 21}}, {'image_id': 23, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 6, 7, 9, 11, 13, 16, 17, 22}}, {'image_id': 24, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 7, 9, 13, 17, 18}}, {'image_id': 25, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 8, 9, 16, 17, 18, 21}}, {'image_id': 26, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 4, 5, 6, 7, 9, 11, 16, 18, 19, 21, 22}}, {'image_id': 27, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {16, 9, 19, 13}}, {'image_id': 28, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 13, 19, 5}}, {'image_id': 29, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 7, 9, 12, 14, 16, 18, 22}}, {'image_id': 30, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 13, 16, 22}}, {'image_id': 31, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 5, 7, 9, 11, 13, 14, 17, 18}}, {'image_id': 32, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 13, 18, 19}}, {'image_id': 33, 'predicted_missing_parts': {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 7, 8, 9, 13, 16, 18, 19}}, {'image_id': 34, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}, 'true_missing_parts': {6, 9, 12, 13, 14, 16, 22}}, {'image_id': 35, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 5, 7, 9, 13, 16, 17, 18, 22}}, {'image_id': 36, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 3, 5, 7, 9, 11, 13, 15, 16, 22}}, {'image_id': 37, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 6, 8, 9, 16, 18, 19, 21, 22}}, {'image_id': 38, 'predicted_missing_parts': {1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 7, 17, 18, 19, 21, 22}}, {'image_id': 39, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {5, 8, 9, 11, 13, 18, 19, 22}}]\n",
      "Micro-F1: 0.5196, Macro-F1: 0.4629\n",
      "Miss Rate: 0.0293, FPPI: 13.5500\n",
      "Overall Acc: 0.3739, Precision: 0.3548, Recall: 0.9707, F1: 0.5196\n",
      "╒═════════════════╤═══════╤════════╤═══════╤═══════╕\n",
      "│ Part            │   Acc │   Prec │   Rec │    F1 │\n",
      "╞═════════════════╪═══════╪════════╪═══════╪═══════╡\n",
      "│ back_pedal      │ 0.6   │  0.6   │ 1     │ 0.75  │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_hand_break │ 0.425 │  0.425 │ 1     │ 0.596 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_pedal     │ 0.075 │  0.026 │ 1     │ 0.051 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_mudguard  │ 0.05  │  0.05  │ 1     │ 0.095 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_light     │ 0.45  │  0.45  │ 1     │ 0.621 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_mudguard   │ 0.375 │  0.219 │ 1     │ 0.359 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_handbreak │ 0.45  │  0.45  │ 1     │ 0.621 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ saddle          │ 0.4   │  0.091 │ 0.333 │ 0.143 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ dress_guard     │ 0.85  │  0.85  │ 1     │ 0.919 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_wheel      │ 0     │  0     │ 0     │ 0     │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ lock            │ 0.425 │  0.425 │ 1     │ 0.596 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_handle     │ 0.175 │  0.175 │ 1     │ 0.298 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ chain           │ 0.6   │  0.6   │ 1     │ 0.75  │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_handle    │ 0.15  │  0.15  │ 1     │ 0.261 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_wheel     │ 0.025 │  0.025 │ 1     │ 0.049 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_reflector  │ 0.55  │  0.545 │ 0.857 │ 0.667 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ kickstand       │ 0.4   │  0.4   │ 1     │ 0.571 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ bell            │ 0.625 │  0.625 │ 1     │ 0.769 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_light      │ 0.6   │  0.6   │ 1     │ 0.75  │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ steer           │ 0.075 │  0.051 │ 1     │ 0.098 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ gear_case       │ 0.375 │  0.361 │ 0.867 │ 0.51  │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ dynamo          │ 0.55  │  0.55  │ 1     │ 0.71  │\n",
      "╘═════════════════╧═══════╧════════╧═══════╧═══════╛\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "results_per_image = evaluate_model(model, valid_loader, train_dataset.part_to_idx, device)\n",
    "\n",
    "print(results_per_image)\n",
    "\n",
    "part_level_evaluation(\n",
    "    results_per_image, train_dataset.part_to_idx, train_dataset.idx_to_part\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
