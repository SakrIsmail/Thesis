{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722bff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data as GraphData\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown\n",
    "from codecarbon import EmissionsTracker\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import dense_to_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95333ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % (2**32)\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fab47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_json='../../data/processed/final_annotations_without_occluded.json'\n",
    "image_directory = '../../data/images'\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "random_seed = 42\n",
    "\n",
    "with open(final_output_json, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_filenames = list(annotations['images'].keys())[:100]\n",
    "\n",
    "random.seed(random_seed)\n",
    "random.shuffle(image_filenames)\n",
    "\n",
    "num_test = int(len(image_filenames) * test_ratio)\n",
    "test_images = image_filenames[:num_test]\n",
    "train_images = image_filenames[num_test:]\n",
    "num_valid = int(len(train_images) * valid_ratio)\n",
    "valid_images = train_images[:num_valid]\n",
    "\n",
    "train_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in train_images}\n",
    "}\n",
    "\n",
    "valid_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in valid_images}\n",
    "}\n",
    "\n",
    "test_annotations = {\n",
    "    'all_parts': annotations['all_parts'],\n",
    "    'images': {img_name: annotations['images'][img_name] for img_name in test_images}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f76445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikePartsDetectionDataset(Dataset):\n",
    "    def __init__(self, annotations_dict, image_dir, transform=None, augment=True, target_size=(640, 640)):\n",
    "        self.all_parts = annotations_dict['all_parts']\n",
    "        self.part_to_idx = {part: idx + 1 for idx, part in enumerate(self.all_parts)}\n",
    "        self.idx_to_part = {idx + 1: part for idx, part in enumerate(self.all_parts)}\n",
    "        self.image_data = annotations_dict['images']\n",
    "        self.image_filenames = list(self.image_data.keys())\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames) * (2 if self.augment else 1)\n",
    "\n",
    "    def apply_augmentation(self, image, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            w = image.width\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_brightness(image, brightness_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.8:\n",
    "            image = transforms.functional.adjust_contrast(image, contrast_factor=random.uniform(0.6, 1.4))\n",
    "        if random.random() < 0.5:\n",
    "            image = transforms.functional.adjust_saturation(image, saturation_factor=random.uniform(0.7, 1.3))\n",
    "\n",
    "        return image, boxes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.image_filenames)\n",
    "        do_augment = self.augment and (idx >= len(self.image_filenames))\n",
    "\n",
    "        img_filename = self.image_filenames[real_idx]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_width, orig_height = image.size\n",
    "\n",
    "        annotation = self.image_data[img_filename]\n",
    "        available_parts_info = annotation['available_parts']\n",
    "        missing_parts_names = annotation.get('missing_parts', [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for part_info in available_parts_info:\n",
    "            part_name = part_info['part_name']\n",
    "            bbox = part_info['absolute_bounding_box']\n",
    "            xmin = bbox['left']\n",
    "            ymin = bbox['top']\n",
    "            xmax = xmin + bbox['width']\n",
    "            ymax = ymin + bbox['height']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.part_to_idx[part_name])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        if do_augment:\n",
    "            image, boxes = self.apply_augmentation(image, boxes)\n",
    "\n",
    "        image = transforms.functional.resize(image, self.target_size)\n",
    "        new_width, new_height = self.target_size\n",
    "        scale_x = new_width / orig_width\n",
    "        scale_y = new_height / orig_height\n",
    "        boxes[:, [0, 2]] *= scale_x\n",
    "        boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "\n",
    "        missing_labels = torch.tensor(\n",
    "            [self.part_to_idx[part] for part in missing_parts_names],\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'missing_labels': missing_labels,\n",
    "            'image_id': torch.tensor([real_idx])\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0904d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=train_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "valid_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=valid_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = BikePartsDetectionDataset(\n",
    "    annotations_dict=test_annotations,\n",
    "    image_dir=image_directory,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef969e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, part_to_idx, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_parts_set = set(part_to_idx.values())\n",
    "    results_per_image = []\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            pred_parts = set(predictions[i]['labels'].cpu().numpy().tolist())\n",
    "            true_missing_parts = set(targets[i]['missing_labels'].cpu().numpy().tolist())\n",
    "            image_id = targets[i]['image_id'].item()\n",
    "\n",
    "            predicted_missing_parts = all_parts_set - pred_parts\n",
    "\n",
    "            results_per_image.append({\n",
    "                'image_id': image_id,\n",
    "                'predicted_missing_parts': predicted_missing_parts,\n",
    "                'true_missing_parts': true_missing_parts\n",
    "            })\n",
    "\n",
    "    return results_per_image\n",
    "\n",
    "\n",
    "def part_level_evaluation(results, part_to_idx, idx_to_part):\n",
    "    parts = list(part_to_idx.values())\n",
    "\n",
    "    Y_true = np.array([[1 if p in r['true_missing_parts'] else 0 for p in parts] for r in results])\n",
    "    Y_pred = np.array([[1 if p in r['predicted_missing_parts'] else 0 for p in parts] for r in results])\n",
    "\n",
    "    micro_f1 = f1_score(Y_true, Y_pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(Y_true, Y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    FN = np.logical_and(Y_true==1, Y_pred==0).sum()\n",
    "    TP = np.logical_and(Y_true==1, Y_pred==1).sum()\n",
    "    FP = np.logical_and(Y_true==0, Y_pred==1).sum()\n",
    "\n",
    "    N_images = len(results)\n",
    "    miss_rate = FN/(FN+TP) if (FN+TP)>0 else 0\n",
    "    fppi = FP/N_images\n",
    "\n",
    "    overall_acc = accuracy_score(Y_true.flatten(), Y_pred.flatten())\n",
    "    overall_prec = precision_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_rec = recall_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_f1 = f1_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    print(f\"Micro-F1: {micro_f1:.4f}, Macro-F1: {macro_f1:.4f}\")\n",
    "    print(f\"Miss Rate: {miss_rate:.4f}, FPPI: {fppi:.4f}\")\n",
    "    print(f\"Overall Acc: {overall_acc:.4f}, Precision: {overall_prec:.4f}, Recall: {overall_rec:.4f}, F1: {overall_f1:.4f}\")\n",
    "    \n",
    "    table=[]\n",
    "    for j,p in enumerate(parts):\n",
    "        acc = accuracy_score(Y_true[:,j], Y_pred[:,j])\n",
    "        prec = precision_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        rec = recall_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        f1s = f1_score(Y_true[:,j], Y_pred[:,j], zero_division=0)\n",
    "        table.append([idx_to_part[p], f\"{acc:.3f}\", f\"{prec:.3f}\", f\"{rec:.3f}\", f\"{f1s:.3f}\"])\n",
    "    print(tabulate(table, headers=[\"Part\",\"Acc\",\"Prec\",\"Rec\",\"F1\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn, FastRCNNPredictor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "class GNNHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GraphRCNNWithDualPrediction(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_channels=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.detector = fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "        in_features = self.detector.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.detector.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        self.gnn_head = GNNHead(in_channels=in_features, hidden_channels=hidden_channels, out_channels=num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        # Consider using CrossEntropyLoss as the GNN is predicting class labels directly\n",
    "        self.gnn_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training:\n",
    "            detector_losses = self.detector(images, targets)\n",
    "\n",
    "            # Remove the no_grad context\n",
    "            image_list, _ = self.detector.transform(images, targets)\n",
    "            features = self.detector.backbone(image_list.tensors)\n",
    "            image_shapes = image_list.image_sizes\n",
    "            proposals = [t[\"boxes\"].detach() for t in targets]\n",
    "\n",
    "            roi_pooled = self.detector.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "            roi_features = self.detector.roi_heads.box_head(roi_pooled)\n",
    "\n",
    "            gnn_losses = 0.0\n",
    "            start = 0\n",
    "\n",
    "            for i, boxes in enumerate(proposals):\n",
    "                num_nodes = boxes.shape[0]\n",
    "                if num_nodes == 0:\n",
    "                    continue\n",
    "\n",
    "                roi_feat = roi_features[start:start + num_nodes]\n",
    "                edge_index = self._build_fully_connected_edges(num_nodes).to(roi_feat.device)\n",
    "\n",
    "                gnn_logits = self.gnn_head(roi_feat, edge_index)\n",
    "                part_labels = targets[i][\"labels\"].to(roi_feat.device)\n",
    "                gnn_loss = self.gnn_loss_fn(gnn_logits, part_labels)\n",
    "                gnn_losses += gnn_loss\n",
    "                start += num_nodes\n",
    "\n",
    "            total_loss = sum(detector_losses.values()) + gnn_losses\n",
    "\n",
    "            return {\n",
    "                **detector_losses,\n",
    "                \"loss_gnn\": gnn_losses,\n",
    "                \"loss_total\": total_loss\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            detections = self.detector(images)\n",
    "            all_outputs = []\n",
    "\n",
    "            for img_idx, det in enumerate(detections):\n",
    "                boxes = det[\"boxes\"]\n",
    "\n",
    "                if boxes.shape[0] == 0:\n",
    "                    all_outputs.append({\n",
    "                        \"boxes\": boxes,\n",
    "                        \"scores\": det[\"scores\"],\n",
    "                        \"labels\": torch.tensor([], dtype=torch.long, device=boxes.device)\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                features = self._get_roi_features(images[img_idx].unsqueeze(0), boxes)\n",
    "                num_nodes = features.shape[0]\n",
    "                edge_index = self._build_fully_connected_edges(num_nodes).to(features.device)\n",
    "                gnn_logits = self.gnn_head(features, edge_index)\n",
    "\n",
    "                # The GNN predicts logits for each class for each detected box\n",
    "                per_box_labels = gnn_logits.argmax(dim=1)\n",
    "\n",
    "                all_outputs.append({\n",
    "                    \"boxes\": boxes,\n",
    "                    \"scores\": det[\"scores\"],\n",
    "                    \"labels\": per_box_labels # Use the GNN's predicted labels\n",
    "                })\n",
    "\n",
    "            return all_outputs\n",
    "\n",
    "    def _get_roi_features(self, image, boxes):\n",
    "        with torch.no_grad():\n",
    "            features = self.detector.backbone(image.tensors if hasattr(image, 'tensors') else image)\n",
    "\n",
    "        image_shapes = [img.shape[-2:] for img in image]\n",
    "        roi_pooled = self.detector.roi_heads.box_roi_pool(features, [boxes], image_shapes)\n",
    "        roi_features = self.detector.roi_heads.box_head(roi_pooled)\n",
    "        return roi_features\n",
    "\n",
    "    def _build_fully_connected_edges(self, num_nodes):\n",
    "        # Creates a fully connected graph where every node is connected to every other node\n",
    "        if num_nodes <= 1:\n",
    "            return torch.empty((2, 0), dtype=torch.long)  # Handle cases with 0 or 1 node\n",
    "        row, col = torch.meshgrid(torch.arange(num_nodes), torch.arange(num_nodes))\n",
    "        edge_index = torch.stack([row.flatten(), col.flatten()], dim=0)\n",
    "        # Remove self-loops\n",
    "        edge_index = edge_index[:, edge_index[0] != edge_index[1]]\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7098798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 20:16:47] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "Epoch 1/1:   0%|          | 0/20 [00:00<?, ?batch/s]/home/sakr/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Epoch 1/1: 100%|██████████| 20/20 [02:17<00:00,  6.87s/batch, loss=28.0696, time (s)=6.827, GPU Mem (MB)=0, CPU Mem (MB)=9745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------+\n",
      "|            Metric             |   Value    |\n",
      "+-------------------------------+------------+\n",
      "|             Epoch             |     1      |\n",
      "|          Final Loss           |  28.0696   |\n",
      "|   Average Batch Time (sec)    |   6.6239   |\n",
      "| Average GPU Memory Usage (MB) |    0.00    |\n",
      "| Average CPU Memory Usage (MB) |  9876.81   |\n",
      "|   Energy Consumption (kWh)    | 0.0007 kWh |\n",
      "|      CO₂ Emissions (kg)       | 0.0002 kg  |\n",
      "+-------------------------------+------------+\n",
      "\n",
      "Evaluating on validation set after Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model (macro-F1: 0.4713)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = len(train_dataset.all_parts) + 1\n",
    "model = GraphRCNNWithDualPrediction(num_classes=num_classes, hidden_channels=512)\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "num_epochs = 1\n",
    "best_macro_f1 = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 3\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    with EmissionsTracker(log_level=\"critical\", save_to_file=False) as tracker:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        batch_times = []\n",
    "        gpu_memories = []\n",
    "        cpu_memories = []\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as tepoch:\n",
    "            for images, targets in tepoch:\n",
    "                start_time = time.time()\n",
    "\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "\n",
    "\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                end_time = time.time()\n",
    "                inference_time = end_time - start_time\n",
    "                batch_times.append(inference_time)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                    gpu_mem_used = mem_info.used / (1024 ** 2)\n",
    "                    gpu_memories.append(gpu_mem_used)\n",
    "                else:\n",
    "                    gpu_mem_used = 0\n",
    "\n",
    "                cpu_mem_used = psutil.virtual_memory().used / (1024 ** 2)\n",
    "                cpu_memories.append(cpu_mem_used)\n",
    "\n",
    "                tepoch.set_postfix({\n",
    "                    \"loss\": f\"{losses.item():.4f}\",\n",
    "                    \"time (s)\": f\"{inference_time:.3f}\",\n",
    "                    \"GPU Mem (MB)\": f\"{gpu_mem_used:.0f}\",\n",
    "                    \"CPU Mem (MB)\": f\"{cpu_mem_used:.0f}\"\n",
    "                })\n",
    "\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    energy_consumption = tracker.final_emissions_data.energy_consumed\n",
    "    co2_emissions = tracker.final_emissions\n",
    "\n",
    "    avg_time = sum(batch_times) / len(batch_times)\n",
    "    max_gpu_mem = max(gpu_memories) if gpu_memories else 0\n",
    "    max_cpu_mem = max(cpu_memories)\n",
    "\n",
    "    table = [\n",
    "        [\"Epoch\", epoch + 1],\n",
    "        [\"Final Loss\", f\"{losses.item():.4f}\"],\n",
    "        [\"Average Batch Time (sec)\", f\"{avg_time:.4f}\"],\n",
    "        [\"Average GPU Memory Usage (MB)\", f\"{max_gpu_mem:.2f}\"],\n",
    "        [\"Average CPU Memory Usage (MB)\", f\"{max_cpu_mem:.2f}\"],\n",
    "        [\"Energy Consumption (kWh)\", f\"{energy_consumption:.4f} kWh\"],\n",
    "        [\"CO₂ Emissions (kg)\", f\"{co2_emissions:.4f} kg\"],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))\n",
    "\n",
    "    print(f\"\\nEvaluating on validation set after Epoch {epoch + 1}...\")\n",
    "    results_per_image = evaluate_model(model, valid_loader, train_dataset.part_to_idx, device)\n",
    "\n",
    "    parts = list(train_dataset.part_to_idx.values())\n",
    "    Y_true = np.array([[1 if p in r['true_missing_parts'] else 0 for p in parts] for r in results_per_image])\n",
    "    Y_pred = np.array([[1 if p in r['predicted_missing_parts'] else 0 for p in parts] for r in results_per_image])\n",
    "    macro_f1 = f1_score(Y_true, Y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), f\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\")\n",
    "        print(f\"Saved new best model (macro-F1: {macro_f1:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement in macro-F1 for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67fbcd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6908/349410656.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\", map_location=device))\n",
      "Evaluating: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 0, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 9, 12, 13, 16, 18, 20, 22}}, {'image_id': 1, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {5, 9, 13, 18, 19, 22}}, {'image_id': 2, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {5, 11, 13, 16, 18, 19}}, {'image_id': 3, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 7, 8, 9, 11, 13, 17, 19, 22}}, {'image_id': 4, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {2, 7, 9, 11, 17, 18, 21, 22}}, {'image_id': 5, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22}, 'true_missing_parts': {1, 5, 6, 9, 13, 14, 16, 19, 22}}, {'image_id': 6, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 9, 13, 18, 19}}, {'image_id': 7, 'predicted_missing_parts': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22}, 'true_missing_parts': {1, 2, 5, 8, 9, 12, 18, 19, 21}}]\n",
      "Micro-F1: 0.5517, Macro-F1: 0.4713\n",
      "Miss Rate: 0.0000, FPPI: 13.0000\n",
      "Overall Acc: 0.4091, Precision: 0.3810, Recall: 1.0000, F1: 0.5517\n",
      "╒═════════════════╤═══════╤════════╤═══════╤═══════╕\n",
      "│ Part            │   Acc │   Prec │   Rec │    F1 │\n",
      "╞═════════════════╪═══════╪════════╪═══════╪═══════╡\n",
      "│ back_pedal      │ 0.625 │  0.625 │     1 │ 0.769 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_hand_break │ 0.5   │  0.5   │     1 │ 0.667 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_pedal     │ 0     │  0     │     0 │ 0     │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_mudguard  │ 0     │  0     │     0 │ 0     │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_light     │ 0.75  │  0.75  │     1 │ 0.857 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_mudguard   │ 0.125 │  0.125 │     1 │ 0.222 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_handbreak │ 0.25  │  0.25  │     1 │ 0.4   │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ saddle          │ 0.25  │  0.25  │     1 │ 0.4   │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ dress_guard     │ 0.875 │  0.875 │     1 │ 0.933 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_wheel      │ 0     │  0     │     0 │ 0     │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ lock            │ 0.375 │  0.375 │     1 │ 0.545 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_handle     │ 0.25  │  0.25  │     1 │ 0.4   │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ chain           │ 0.75  │  0.75  │     1 │ 0.857 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_handle    │ 0.125 │  0.125 │     1 │ 0.222 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ front_wheel     │ 0.875 │  0     │     0 │ 0     │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_reflector  │ 0.375 │  0.375 │     1 │ 0.545 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ kickstand       │ 0.25  │  0.25  │     1 │ 0.4   │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ bell            │ 0.75  │  0.75  │     1 │ 0.857 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ back_light      │ 0.75  │  0.75  │     1 │ 0.857 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ steer           │ 0.125 │  0.125 │     1 │ 0.222 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ gear_case       │ 0.375 │  0.286 │     1 │ 0.444 │\n",
      "├─────────────────┼───────┼────────┼───────┼───────┤\n",
      "│ dynamo          │ 0.625 │  0.625 │     1 │ 0.769 │\n",
      "╘═════════════════╧═══════╧════════╧═══════╧═══════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../../models/graph_rcnn/graphrcnn_MobileNet_baseline_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "results_per_image = evaluate_model(model, valid_loader, train_dataset.part_to_idx, device)\n",
    "\n",
    "print(results_per_image)\n",
    "\n",
    "part_level_evaluation(\n",
    "    results_per_image, train_dataset.part_to_idx, train_dataset.idx_to_part\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
