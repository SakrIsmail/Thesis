{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a257d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f301a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelftBikesGraphDataset(Dataset):\n",
    "    def __init__(self, annotations_file, image_dir, transform=None, resize=(640, 480)):\n",
    "        with open(annotations_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Resize the image to a fixed size (e.g., 640x480)\n",
    "        self.resize = resize\n",
    "        \n",
    "        # Get all parts names and their corresponding ids\n",
    "        self.part_names = self.annotations[\"all_parts\"]\n",
    "        self.part2id = {name: idx + 1 for idx, name in enumerate(self.part_names)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations) - 1  # Exclude \"all_parts\" key\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = list(self.annotations.keys())[idx + 1]  # Skip the \"all_parts\" key\n",
    "        image_data = self.annotations[image_name]\n",
    "        \n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize(self.resize)  # Resize to the fixed size (640, 480)\n",
    "        \n",
    "        # Extract parts and create boxes and labels\n",
    "        target = {}\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for part in image_data[\"available_parts\"]:\n",
    "            part_name = part[\"part_name\"]\n",
    "            relative_bbox = part[\"relative_bounding_box\"]\n",
    "            \n",
    "            # Convert relative bbox to absolute coordinates\n",
    "            x_min = relative_bbox[\"left\"] * image.width\n",
    "            y_min = relative_bbox[\"top\"] * image.height\n",
    "            width = relative_bbox[\"width\"] * image.width\n",
    "            height = relative_bbox[\"height\"] * image.height\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(self.part2id[part_name])\n",
    "        \n",
    "        # Padding for bounding boxes (to ensure equal length)\n",
    "        max_boxes = max([len(image_data[\"available_parts\"]) for image_data in self.annotations.values()])\n",
    "        padding = max_boxes - len(boxes)\n",
    "        \n",
    "        # Pad the boxes with zeros if needed\n",
    "        if padding > 0:\n",
    "            boxes.extend([[0, 0, 0, 0]] * padding)  # Pad with dummy boxes of [0, 0, 0, 0]\n",
    "            labels.extend([0] * padding)  # Pad with dummy labels (0)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target[\"boxes\"] = torch.tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Transform the image if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create empty graph data for this image (we don't use graph edges here)\n",
    "        edge_index = torch.tensor([[], []], dtype=torch.long)  # No edges in this basic setup\n",
    "        x = torch.ones(len(boxes), dtype=torch.float32)  # Each box has a feature vector (dummy here)\n",
    "\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        \n",
    "        return image, target, graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "675ae416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRCNN(nn.Module):\n",
    "    def __init__(self, num_classes, num_parts):\n",
    "        super(GraphRCNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define GCN layers\n",
    "        self.gcn1 = GCNConv(num_parts, 64)\n",
    "        self.gcn2 = GCNConv(64, 128)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Bounding box regression layers\n",
    "        self.bbox_fc1 = nn.Linear(128, 128)\n",
    "        self.bbox_fc2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, images, edge_index):\n",
    "        # Graph convolution layers\n",
    "        x = self.gcn1(images.x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Classifier\n",
    "        class_out = self.fc1(x)\n",
    "        class_out = F.relu(class_out)\n",
    "        class_out = self.fc2(class_out)\n",
    "\n",
    "        # Bounding box regression\n",
    "        bbox_out = self.bbox_fc1(x)\n",
    "        bbox_out = F.relu(bbox_out)\n",
    "        bbox_out = self.bbox_fc2(bbox_out)\n",
    "\n",
    "        return {\"labels\": class_out, \"boxes\": bbox_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e52619fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets):\n",
    "    pred_labels = predictions['labels']\n",
    "    pred_boxes = predictions['boxes']\n",
    "    \n",
    "    gt_labels = targets['labels']\n",
    "    gt_boxes = targets['boxes']\n",
    "    \n",
    "    classification_loss = F.cross_entropy(pred_labels, gt_labels)\n",
    "    bbox_loss = smooth_l1_loss(pred_boxes, gt_boxes)\n",
    "    \n",
    "    total_loss = classification_loss + bbox_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def smooth_l1_loss(pred_boxes, gt_boxes, beta=1.0):\n",
    "    diff = torch.abs(pred_boxes - gt_boxes)\n",
    "    loss = torch.where(diff < beta, 0.5 * diff ** 2, beta * (diff - 0.5 * beta))\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2667be44",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m     15\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    672\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    675\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mDelftBikesGraphDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     47\u001b[39m     labels.append(\u001b[38;5;28mself\u001b[39m.part2id[part_name])\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Padding for bounding boxes (to ensure equal length)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m max_boxes = \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(\u001b[43mimage_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mavailable_parts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m image_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.annotations.values()])\n\u001b[32m     51\u001b[39m padding = max_boxes - \u001b[38;5;28mlen\u001b[39m(boxes)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Pad the boxes with zeros if needed\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "train_dataset = DelftBikesGraphDataset(annotations_file='new_annotations.json', image_dir='DelftBikes/train', transform=transform, resize=(640, 480))\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphRCNN(num_classes=len(train_dataset.part_names) + 1, num_parts=len(train_dataset.part_names))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Move model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for images, targets, graph_data in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        edge_index = graph_data.edge_index.to(device)\n",
    "        x = graph_data.x.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model(images, edge_index)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(predictions, targets)\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ae203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_find_missing(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets, graph_data in dataloader:\n",
    "            images = images.to(device)\n",
    "            predictions = model(images, graph_data.edge_index.to(device))\n",
    "            \n",
    "            for idx in range(len(images)):\n",
    "                # Get the detected labels (predicted parts)\n",
    "                detected_labels = predictions[idx]['labels'].cpu().numpy()\n",
    "                detected_parts = [label_to_part(label) for label in detected_labels]\n",
    "                \n",
    "                # Subtract detected parts from all parts to find missing ones\n",
    "                missing_parts = list(set(train_dataset.part_names) - set(detected_parts))\n",
    "                print(f\"Missing parts for image {idx + 1}: {missing_parts}\")\n",
    "\n",
    "def label_to_part(label):\n",
    "    return train_dataset.part_names[label - 1]  # Adjust if needed based on your indexing\n",
    "\n",
    "# After training, evaluate on the same training data (or on a validation set if available)\n",
    "evaluate_and_find_missing(model, train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
