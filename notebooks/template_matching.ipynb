{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccbd9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakr/anaconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sakr/anaconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "TileTrain 0:   2%|‚ñè         | 11/449 [00:50<33:19,  4.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 326\u001b[39m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# 9) RUN EVERYTHING\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    324\u001b[39m \n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# 1) train submodules\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m tile_model = \u001b[43mtrain_tile_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m pres_model = train_presence_verifier()\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# 2) inference & evaluation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mtrain_tile_detector\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    236\u001b[39m     pred  = model(xb)\n\u001b[32m    237\u001b[39m     l     = F.binary_cross_entropy(pred, yb)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     opt.zero_grad(); \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m; opt.step()\n\u001b[32m    239\u001b[39m     tot+=\u001b[32m1\u001b[39m; loss+=l.item()\n\u001b[32m    240\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Epoch\u001b[39m\u001b[33m\"\u001b[39m,epoch,\u001b[33m\"\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m\"\u001b[39m,loss/tot)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    766\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "DATA_JSON = \"../data/processed/final_annotations_without_occluded.json\"\n",
    "IMG_DIR   = \"../data/images\"\n",
    "DEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED      = 42\n",
    "\n",
    "# tile & presence crop sizes\n",
    "TILE_SIZE     = 224\n",
    "PRESENCE_SIZE = 64\n",
    "\n",
    "BATCH_TILE      = 64\n",
    "BATCH_OFFSET    = 32\n",
    "BATCH_PRESENCE  = 128\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) UTILITIES\n",
    "# ----------------------------------------------------------------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) LOAD ANNOTATIONS & SPLIT\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(DATA_JSON, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "images = list(ann[\"images\"].keys())\n",
    "random.shuffle(images)\n",
    "n_test  = int(len(images)*0.2)\n",
    "n_val   = int((len(images)-n_test)*0.1)\n",
    "\n",
    "test_imgs  = images[:n_test]\n",
    "val_imgs   = images[n_test:n_test+n_val]\n",
    "train_imgs = images[n_test+n_val:]\n",
    "\n",
    "def subset(subset_list):\n",
    "    return {\n",
    "        \"all_parts\": ann[\"all_parts\"],\n",
    "        \"images\": {fn: ann[\"images\"][fn] for fn in subset_list}\n",
    "    }\n",
    "\n",
    "train_ann = subset(train_imgs)\n",
    "val_ann   = subset(val_imgs)\n",
    "test_ann  = subset(test_imgs)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) PRECOMPUTE STATIC AVERAGE OFFSETS\n",
    "# ----------------------------------------------------------------------------\n",
    "# Build mapping part_name->idx and vice versa\n",
    "parts       = ann[\"all_parts\"]\n",
    "part2idx    = {p:i for i,p in enumerate(parts)}\n",
    "idx2part    = {i:p for p,i in part2idx.items()}\n",
    "num_parts   = len(parts)\n",
    "\n",
    "# collect all offsets\n",
    "offsets = { seed_i: { tgt_j: [] for tgt_j in range(num_parts) if tgt_j!=seed_i }\n",
    "            for seed_i in range(num_parts) }\n",
    "\n",
    "for fn,info in train_ann[\"images\"].items():\n",
    "    # compute box centers\n",
    "    centers = {}\n",
    "    sizes   = {}\n",
    "    for pi in info[\"available_parts\"]:\n",
    "        idx = part2idx[pi[\"part_name\"]]\n",
    "        bb  = pi[\"absolute_bounding_box\"]\n",
    "        cx  = bb[\"left\"] + bb[\"width\"]/2\n",
    "        cy  = bb[\"top\" ] + bb[\"height\"]/2\n",
    "        centers[idx] = (cx,cy)\n",
    "        sizes[idx]   = (bb[\"width\"], bb[\"height\"])\n",
    "    for s, (cx,cy) in centers.items():\n",
    "        for t,(tx,ty) in centers.items():\n",
    "            if t==s: continue\n",
    "            dx = tx-cx\n",
    "            dy = ty-cy\n",
    "            tw,th = sizes[t]\n",
    "            offsets[s][t].append((dx,dy,tw,th))\n",
    "\n",
    "# average them\n",
    "avg_offsets = {}\n",
    "for s,d in offsets.items():\n",
    "    avg_offsets[s] = {}\n",
    "    for t,lst in d.items():\n",
    "        arr = np.array(lst)\n",
    "        mean = arr.mean(axis=0)\n",
    "        avg_offsets[s][t] = mean  # dx,dy,w,h\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) DATASETS\n",
    "# ----------------------------------------------------------------------------\n",
    "class TileDataset(Dataset):\n",
    "    \"\"\"Produces random (or center) tiles with multi-label parts.\"\"\"\n",
    "    def __init__(self, annotations, image_dir, tile_size=TILE_SIZE, tiles_per_img=5):\n",
    "        self.images     = list(annotations[\"images\"].keys())\n",
    "        self.image_dir  = image_dir\n",
    "        self.tile_size  = tile_size\n",
    "        self.tiles_pi   = tiles_per_img\n",
    "        self.ann        = annotations[\"images\"]\n",
    "        self.tf         = transforms.Compose([\n",
    "            transforms.Resize((tile_size,tile_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.images)*self.tiles_pi\n",
    "    def __getitem__(self, idx):\n",
    "        im_idx = idx // self.tiles_pi\n",
    "        fn     = self.images[im_idx]\n",
    "        img    = Image.open(os.path.join(self.image_dir,fn)).convert(\"RGB\")\n",
    "        W,H    = img.size\n",
    "\n",
    "        # sample center tile for first, then random\n",
    "        if idx % self.tiles_pi == 0:\n",
    "            x0,y0 = (W-self.tile_size)//2,(H-self.tile_size)//2\n",
    "        else:\n",
    "            x0 = random.randint(0, max(0,W-self.tile_size))\n",
    "            y0 = random.randint(0, max(0,H-self.tile_size))\n",
    "\n",
    "        crop = img.crop((x0,y0,x0+self.tile_size,y0+self.tile_size))\n",
    "        label = torch.zeros(num_parts,dtype=torch.float32)\n",
    "        for pi in self.ann[fn][\"available_parts\"]:\n",
    "            pidx = part2idx[pi[\"part_name\"]]\n",
    "            bb   = pi[\"absolute_bounding_box\"]\n",
    "            cx   = bb[\"left\"]+bb[\"width\"]/2\n",
    "            cy   = bb[\"top\"] +bb[\"height\"]/2\n",
    "            if x0<=cx<=x0+self.tile_size and y0<=cy<=y0+self.tile_size:\n",
    "                label[pidx] = 1.0\n",
    "        return self.tf(crop), label\n",
    "\n",
    "class PresenceDataset(Dataset):\n",
    "    \"\"\"Crops predicted boxes + binary present/missing label.\"\"\"\n",
    "    def __init__(self, annotations, image_dir, avg_offsets, tile_size=PRESENCE_SIZE):\n",
    "        self.images     = list(annotations[\"images\"].keys())\n",
    "        self.ann        = annotations[\"images\"]\n",
    "        self.image_dir  = image_dir\n",
    "        self.avg_off    = avg_offsets\n",
    "        self.tile_size  = tile_size\n",
    "        self.tf         = transforms.Compose([\n",
    "            transforms.Resize((tile_size,tile_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.items = []\n",
    "        # prepare all (fn, seed, target, present_flag)\n",
    "        for fn,info in self.ann.items():\n",
    "            # collect seed centers\n",
    "            seeds = []\n",
    "            for pi in info[\"available_parts\"]:\n",
    "                si = part2idx[pi[\"part_name\"]]\n",
    "                bb = pi[\"absolute_bounding_box\"]\n",
    "                cx,cy = bb[\"left\"]+bb[\"width\"]/2, bb[\"top\"]+bb[\"height\"]/2\n",
    "                seeds.append((si,cx,cy))\n",
    "            # for each seed, each target part\n",
    "            for si,cx,cy in seeds:\n",
    "                for ti in range(num_parts):\n",
    "                    dx,dy,tw,th = self.avg_off[si].get(ti,(0,0,0,0))\n",
    "                    x0 = int(cx+dx - tw/2)\n",
    "                    y0 = int(cy+dy - th/2)\n",
    "                    self.items.append((fn,ti,x0,y0))\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        fn,ti,x0,y0 = self.items[idx]\n",
    "        img = Image.open(os.path.join(self.image_dir,fn)).convert(\"RGB\")\n",
    "        W,H = img.size\n",
    "        # clamp\n",
    "        x0 = max(0, min(x0, W-self.tile_size))\n",
    "        y0 = max(0, min(y0, H-self.tile_size))\n",
    "        crop = img.crop((x0,y0, x0+self.tile_size, y0+self.tile_size))\n",
    "        label = 0\n",
    "        # check if truly present in GT\n",
    "        for pi in self.ann[fn][\"available_parts\"]:\n",
    "            if part2idx[pi[\"part_name\"]] == ti:\n",
    "                label = 1\n",
    "        return self.tf(crop), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) MODELS\n",
    "# ----------------------------------------------------------------------------\n",
    "class TileNet(nn.Module):\n",
    "    def __init__(self, num_parts):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_parts)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.backbone(x))\n",
    "\n",
    "class PresenceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc   = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(64*(PRESENCE_SIZE//4)**2,128),\n",
    "            nn.ReLU(), nn.Linear(128,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(self.conv(x))).squeeze(1)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7) TRAINING FUNCTIONS\n",
    "# ----------------------------------------------------------------------------\n",
    "def train_tile_detector():\n",
    "    ds = TileDataset(train_ann, IMG_DIR)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_TILE, shuffle=True, num_workers=4)\n",
    "    model = TileNet(num_parts).to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        tot,loss=0,0\n",
    "        for xb,yb in tqdm(dl, desc=f\"TileTrain {epoch}\"):\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred  = model(xb)\n",
    "            l     = F.binary_cross_entropy(pred, yb)\n",
    "            opt.zero_grad(); l.backward(); opt.step()\n",
    "            tot+=1; loss+=l.item()\n",
    "        print(\" Epoch\",epoch,\"Loss\",loss/tot)\n",
    "    torch.save(model.state_dict(),\"tile_net.pth\")\n",
    "    return model\n",
    "\n",
    "def train_presence_verifier():\n",
    "    ds = PresenceDataset(train_ann, IMG_DIR, avg_offsets)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_PRESENCE, shuffle=True, num_workers=4)\n",
    "    model = PresenceNet().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        tot,loss=0,0\n",
    "        for xb,yb in tqdm(dl, desc=f\"PresTrain {epoch}\"):\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred  = model(xb)\n",
    "            l     = F.binary_cross_entropy(pred, yb)\n",
    "            opt.zero_grad(); l.backward(); opt.step()\n",
    "            tot+=1; loss+=l.item()\n",
    "        print(\" Epoch\",epoch,\"Loss\",loss/tot)\n",
    "    torch.save(model.state_dict(),\"pres_net.pth\")\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8) INFERENCE PIPELINE\n",
    "# ----------------------------------------------------------------------------\n",
    "def inference_on_split(split_ann):\n",
    "    tile_ds = TileDataset(split_ann, IMG_DIR, tiles_per_img=1)\n",
    "    pres_model = PresenceNet().to(DEVICE)\n",
    "    pres_model.load_state_dict(torch.load(\"pres_net.pth\"))\n",
    "    pres_model.eval()\n",
    "\n",
    "    tile_model = TileNet(num_parts).to(DEVICE)\n",
    "    tile_model.load_state_dict(torch.load(\"tile_net.pth\"))\n",
    "    tile_model.eval()\n",
    "\n",
    "    results = []\n",
    "    for fn in tqdm(split_ann[\"images\"].keys(), desc=\"Inf images\"):\n",
    "        img = Image.open(os.path.join(IMG_DIR,fn)).convert(\"RGB\")\n",
    "        W,H = img.size\n",
    "\n",
    "        # 1) get central tile\n",
    "        x0,y0 = (W-TILE_SIZE)//2,(H-TILE_SIZE)//2\n",
    "        crop = img.crop((x0,y0,x0+TILE_SIZE,y0+TILE_SIZE))\n",
    "        inp  = transforms.ToTensor()(transforms.Resize((TILE_SIZE,TILE_SIZE))(crop)).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # 2) detect seeds\n",
    "        with torch.no_grad():\n",
    "            preds = tile_model(inp)[0].cpu().numpy()\n",
    "        seeds = [(i,p) for i,p in enumerate(preds) if p>0.5]\n",
    "\n",
    "        # if no seeds, treat all missing\n",
    "        if not seeds:\n",
    "            missing = set(range(num_parts))\n",
    "            results.append((fn,missing)); continue\n",
    "\n",
    "        # 3) for every seed, estimate each part box, run presence net\n",
    "        part_conf = {i:0.0 for i in range(num_parts)}\n",
    "        for si,conf in seeds:\n",
    "            # assume seed center is tile-center\n",
    "            cx = x0+TILE_SIZE/2; cy=y0+TILE_SIZE/2\n",
    "            for ti in range(num_parts):\n",
    "                if ti==si: \n",
    "                    part_conf[ti] = max(part_conf[ti], conf)\n",
    "                    continue\n",
    "                dx,dy,w,h = avg_offsets[si].get(ti,(0,0,0,0))\n",
    "                x1 = int(cx+dx - PRESENCE_SIZE/2)\n",
    "                y1 = int(cy+dy - PRESENCE_SIZE/2)\n",
    "                x1 = max(0,min(x1,W-PRESENCE_SIZE))\n",
    "                y1 = max(0,min(y1,H-PRESENCE_SIZE))\n",
    "                box_crop = img.crop((x1,y1,x1+PRESENCE_SIZE,y1+PRESENCE_SIZE))\n",
    "                inp2 = transforms.ToTensor()(transforms.Resize((PRESENCE_SIZE,PRESENCE_SIZE))(box_crop)).unsqueeze(0).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    p = pres_model(inp2).item()\n",
    "                part_conf[ti] = max(part_conf[ti], p)\n",
    "\n",
    "        # 4) missing = those with conf < 0.5\n",
    "        missing = {ti for ti,c in part_conf.items() if c < 0.5}\n",
    "        results.append((fn, missing))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 9) RUN EVERYTHING\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "    # 1) train submodules\n",
    "tile_model = train_tile_detector()\n",
    "pres_model = train_presence_verifier()\n",
    "\n",
    "# 2) inference & evaluation\n",
    "for split,ann_split in [(\"VAL\",val_ann),(\"TEST\",test_ann)]:\n",
    "    res = inference_on_split(ann_split)\n",
    "    # compute simple metrics:\n",
    "    Yt, Yp = [], []\n",
    "    for fn,miss in res:\n",
    "        gt = set([part2idx[p] for p in ann_split[\"images\"][fn].get(\"missing_parts\",[])])\n",
    "        Yt.append([1 if i in gt else 0 for i in range(num_parts)])\n",
    "        Yp.append([1 if i in miss else 0 for i in range(num_parts)])\n",
    "    Yt = np.array(Yt); Yp = np.array(Yp)\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    print(f\"{split} Micro-F1:\", f1_score(Yt, Yp, average=\"micro\", zero_division=0))\n",
    "    print(f\"{split} Macro-F1:\", f1_score(Yt, Yp, average=\"macro\", zero_division=0))\n",
    "    print(f\"{split} Precision:\", precision_score(Yt, Yp, average=\"micro\", zero_division=0))\n",
    "    print(f\"{split} Recall:\", recall_score(Yt, Yp, average=\"micro\", zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
