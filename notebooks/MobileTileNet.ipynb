{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccbd9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5742, Val: 638, Test: 1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakr/anaconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sakr/anaconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor part: back_pedal\n",
      "Average anchor position: [323.4758003 289.5394436]\n",
      "Average offsets for parts (example): {'back_pedal': array([0., 0.]), 'back_hand_break': array([ -41.00623624, -164.98789435]), 'front_pedal': array([28.69489194, 19.22082515])}\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:16<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4867, Time: 3.42s, Pixels: 1552269312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4571\n",
      "Micro F1: 0.6432, Macro F1: 0.3644\n",
      "Miss Rate: 0.4080, FPPI: 1.8339\n",
      "Overall Acc: 0.7801, Precision: 0.7039, Recall: 0.5920, F1: 0.6432\n",
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    0    |\n",
      "| Maximum GPU Memory Usage (MB) | 3744.56 |\n",
      "| Maximum CPU Memory Usage (MB) | 9403.96 |\n",
      "+-------------------------------+---------+\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:16<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4427, Time: 3.15s, Pixels: 1552269312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 40.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4411\n",
      "Micro F1: 0.6648, Macro F1: 0.4176\n",
      "Miss Rate: 0.3831, FPPI: 1.7618\n",
      "Overall Acc: 0.7917, Precision: 0.7206, Recall: 0.6169, F1: 0.6648\n",
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    1    |\n",
      "| Maximum GPU Memory Usage (MB) | 3724.88 |\n",
      "| Maximum CPU Memory Usage (MB) | 9203.69 |\n",
      "+-------------------------------+---------+\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:16<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4173, Time: 3.13s, Pixels: 1552269312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 40.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4339\n",
      "Micro F1: 0.6742, Macro F1: 0.4549\n",
      "Miss Rate: 0.3682, FPPI: 1.7868\n",
      "Overall Acc: 0.7955, Precision: 0.7226, Recall: 0.6318, F1: 0.6742\n",
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    2    |\n",
      "| Maximum GPU Memory Usage (MB) | 3711.94 |\n",
      "| Maximum CPU Memory Usage (MB) | 9225.93 |\n",
      "+-------------------------------+---------+\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:16<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3927, Time: 3.17s, Pixels: 1552269312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 36.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4315\n",
      "Micro F1: 0.6791, Macro F1: 0.4636\n",
      "Miss Rate: 0.3709, FPPI: 1.6473\n",
      "Overall Acc: 0.8009, Precision: 0.7377, Recall: 0.6291, F1: 0.6791\n",
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    3    |\n",
      "| Maximum GPU Memory Usage (MB) | 3719.69 |\n",
      "| Maximum CPU Memory Usage (MB) | 9154.45 |\n",
      "+-------------------------------+---------+\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:16<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3676, Time: 3.17s, Pixels: 1552269312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 39.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4349\n",
      "Micro F1: 0.6887, Macro F1: 0.4973\n",
      "Miss Rate: 0.3384, FPPI: 1.9122\n",
      "Overall Acc: 0.7998, Precision: 0.7182, Recall: 0.6616, F1: 0.6887\n",
      "+-------------------------------+---------+\n",
      "|            Metric             |  Value  |\n",
      "+-------------------------------+---------+\n",
      "|             Epoch             |    4    |\n",
      "| Maximum GPU Memory Usage (MB) | 3734.69 |\n",
      "| Maximum CPU Memory Usage (MB) | 9161.47 |\n",
      "+-------------------------------+---------+\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4327\n",
      "Micro F1: 0.6910, Macro F1: 0.4988\n",
      "Miss Rate: 0.3335, FPPI: 1.9204\n",
      "Overall Acc: 0.8019, Precision: 0.7173, Recall: 0.6665, F1: 0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import psutil\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlShutdown,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- SETTINGS --- #\n",
    "SEED = 42\n",
    "TILE_SIZE = 64\n",
    "INPUT_SIZE = (640, 640)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_PARTS = 22  # Adjust if needed\n",
    "\n",
    "# --- SEEDING --- #\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- TRANSFORMS --- #\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((TILE_SIZE, TILE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def visualize_tile_locations(image_path, center, all_parts, offsets):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    cx, cy = center\n",
    "    ax.scatter(cx, cy, c='r', label=\"Anchor\")\n",
    "    for part in all_parts:\n",
    "        dx, dy = offsets.get(part, (0, 0))\n",
    "        px = cx + dx\n",
    "        py = cy + dy\n",
    "        ax.scatter(px, py, alpha=0.7, label=part)\n",
    "        rect = plt.Rectangle((px - TILE_SIZE//2, py - TILE_SIZE//2), TILE_SIZE, TILE_SIZE,\n",
    "                             linewidth=1, edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(\"Estimated Tile Positions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_model_decision(model, dataset, all_parts, image_dir, sample_idx=0):\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    model.eval()\n",
    "    img_name = dataset.images[sample_idx]\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Ground truth\n",
    "    true_labels = dataset.annotations[img_name].get(\"missing_parts\", [])\n",
    "    true_mask = {p: 1 for p in true_labels}\n",
    "\n",
    "    # Get tiles and prediction\n",
    "    tiles, label_tensor = dataset[sample_idx]\n",
    "    tiles = tiles.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tiles)\n",
    "        probs = torch.sigmoid(output).cpu().squeeze().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Anchor is at image center\n",
    "    cx, cy = image.size[0] // 2, image.size[1] // 2\n",
    "    tile_centers = estimate_other_tiles((cx, cy), all_parts, dataset.average_offsets)\n",
    "\n",
    "    # Plot image and predictions\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for i, (part, (px, py)) in enumerate(zip(all_parts, tile_centers)):\n",
    "        pred = preds[i]\n",
    "        true = true_mask.get(part, 0)\n",
    "\n",
    "        # Determine color\n",
    "        if pred == 1 and true == 0:\n",
    "            color = \"blue\"    # False positive\n",
    "        elif pred == 1 and true == 1:\n",
    "            color = \"green\"   # True positive\n",
    "        elif pred == 0 and true == 1:\n",
    "            color = \"red\"     # False negative\n",
    "        else:\n",
    "            color = \"black\"   # True negative\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (px - TILE_SIZE//2, py - TILE_SIZE//2),\n",
    "            TILE_SIZE, TILE_SIZE,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(px, py, part, fontsize=8, color=color, ha='center')\n",
    "\n",
    "    plt.title(f\"Prediction vs Ground Truth for '{img_name}'\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- UTILITIES TO FIND ANCHOR AND AVG OFFSETS --- #\n",
    "\n",
    "def part_in_center_tile(part_bbox, image_size, tile_size=TILE_SIZE):\n",
    "    \"\"\"Check if part bbox center lies inside center tile area.\"\"\"\n",
    "    img_w, img_h = image_size\n",
    "    cx, cy = img_w // 2, img_h // 2\n",
    "    tile_left = cx - tile_size // 2\n",
    "    tile_top = cy - tile_size // 2\n",
    "    tile_right = tile_left + tile_size\n",
    "    tile_bottom = tile_top + tile_size\n",
    "\n",
    "    part_cx = part_bbox['left'] + part_bbox['width'] / 2\n",
    "    part_cy = part_bbox['top'] + part_bbox['height'] / 2\n",
    "\n",
    "    return (tile_left <= part_cx <= tile_right) and (tile_top <= part_cy <= tile_bottom)\n",
    "\n",
    "def find_anchor_part(images_subset, all_parts, image_dir):\n",
    "    anchor_counts = {part: 0 for part in all_parts}\n",
    "    for img_id, img_data in images_subset.items():\n",
    "        img_path = os.path.join(image_dir, img_id)\n",
    "        with Image.open(img_path) as img:\n",
    "            img_w, img_h = img.size\n",
    "\n",
    "        for part in img_data.get('available_parts', []):\n",
    "            name = part['part_name']\n",
    "            bbox = part['absolute_bounding_box']\n",
    "            center_x = bbox['left'] + bbox['width'] / 2\n",
    "            center_y = bbox['top'] + bbox['height'] / 2\n",
    "            # Check if inside center tile\n",
    "            left = img_w // 2 - TILE_SIZE // 2\n",
    "            top = img_h // 2 - TILE_SIZE // 2\n",
    "            if left <= center_x <= left + TILE_SIZE and top <= center_y <= top + TILE_SIZE:\n",
    "                anchor_counts[name] += 1\n",
    "\n",
    "    # Return the part with max count\n",
    "    anchor_part = max(anchor_counts, key=anchor_counts.get)\n",
    "    return anchor_part\n",
    "\n",
    "def load_avg_positions_from_subset_with_anchor(train_images_subset, all_parts, anchor_part, image_dir):\n",
    "    anchor_positions = []\n",
    "    part_positions = {part: [] for part in all_parts}\n",
    "\n",
    "    for img_id, img_data in train_images_subset.items():\n",
    "        # Load image size\n",
    "        img_path = os.path.join(image_dir, img_id)\n",
    "        with Image.open(img_path) as img:\n",
    "            img_w, img_h = img.size\n",
    "\n",
    "        # Find anchor part position in this image\n",
    "        anchor_pos = None\n",
    "        for part in img_data.get('available_parts', []):\n",
    "            name = part['part_name']\n",
    "            bbox = part['absolute_bounding_box']\n",
    "            center_x = bbox['left'] + bbox['width'] / 2\n",
    "            center_y = bbox['top'] + bbox['height'] / 2\n",
    "            if name == anchor_part:\n",
    "                anchor_pos = (center_x, center_y)\n",
    "                break\n",
    "        if anchor_pos is None:\n",
    "            # Skip image if anchor part missing\n",
    "            continue\n",
    "        anchor_positions.append(anchor_pos)\n",
    "        # Record positions relative to anchor\n",
    "        for part in img_data.get('available_parts', []):\n",
    "            name = part['part_name']\n",
    "            bbox = part['absolute_bounding_box']\n",
    "            cx = bbox['left'] + bbox['width'] / 2\n",
    "            cy = bbox['top'] + bbox['height'] / 2\n",
    "            rel_x = cx - anchor_pos[0]\n",
    "            rel_y = cy - anchor_pos[1]\n",
    "            part_positions[name].append((rel_x, rel_y))\n",
    "\n",
    "    avg_anchor_pos = np.mean(anchor_positions, axis=0)\n",
    "    avg_offsets = {}\n",
    "    for part in all_parts:\n",
    "        if part_positions[part]:\n",
    "            avg_offsets[part] = np.mean(part_positions[part], axis=0)\n",
    "        else:\n",
    "            avg_offsets[part] = (0, 0)\n",
    "    return avg_anchor_pos, avg_offsets\n",
    "\n",
    "# --- TILE CROPPING BASED ON ANCHOR OFFSETS --- #\n",
    "\n",
    "def crop_tile(image: Image.Image, center: Tuple[int, int]) -> Image.Image:\n",
    "    cx, cy = center\n",
    "    left = max(cx - TILE_SIZE // 2, 0)\n",
    "    top = max(cy - TILE_SIZE // 2, 0)\n",
    "    right = left + TILE_SIZE\n",
    "    bottom = top + TILE_SIZE\n",
    "    # Ensure crop box inside image bounds\n",
    "    right = min(right, image.width)\n",
    "    bottom = min(bottom, image.height)\n",
    "    left = right - TILE_SIZE\n",
    "    top = bottom - TILE_SIZE\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "def estimate_other_tiles(center: Tuple[int, int], all_parts: List[str], average_offsets: dict):\n",
    "    cx, cy = center\n",
    "    tile_centers = []\n",
    "    for part in all_parts:\n",
    "        dx, dy = average_offsets.get(part, (0, 0))\n",
    "        est_x = int(cx + dx)\n",
    "        est_y = int(cy + dy)\n",
    "        tile_centers.append((est_x, est_y))\n",
    "    return tile_centers\n",
    "\n",
    "# --- DATASET --- #\n",
    "class BikeTileDataset(Dataset):\n",
    "    def __init__(self, annotations, image_dir, image_ids, all_parts, average_offsets, target_size=(640, 640)):\n",
    "        self.image_dir = image_dir\n",
    "        self.images = image_ids\n",
    "        self.annotations = annotations[\"images\"]\n",
    "        self.part_to_idx = {part: i for i, part in enumerate(all_parts)}\n",
    "        self.all_parts = all_parts\n",
    "        self.average_offsets = average_offsets\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        original_w, original_h = image.size\n",
    "        image = image.resize(self.target_size, Image.BILINEAR)\n",
    "        resized_w, resized_h = self.target_size\n",
    "\n",
    "        scale_x = resized_w / original_w\n",
    "        scale_y = resized_h / original_h\n",
    "\n",
    "        parts = self.annotations[img_name].get(\"available_parts\", [])\n",
    "        for part in parts:\n",
    "            bbox = part[\"absolute_bounding_box\"]\n",
    "            bbox[\"left\"] *= scale_x\n",
    "            bbox[\"top\"] *= scale_y\n",
    "            bbox[\"width\"] *= scale_x\n",
    "            bbox[\"height\"] *= scale_y\n",
    "\n",
    "        cx, cy = resized_w // 2, resized_h // 2\n",
    "        tile_centers = estimate_other_tiles((cx, cy), self.all_parts, self.average_offsets)\n",
    "\n",
    "        tiles = [transform(crop_tile(image, center)) for center in tile_centers]\n",
    "        tiles = torch.stack(tiles)\n",
    "\n",
    "        missing_parts = self.annotations[img_name].get(\"missing_parts\", [])\n",
    "        label = torch.zeros(len(self.all_parts))\n",
    "        for part in missing_parts:\n",
    "            idx_part = self.part_to_idx[part]\n",
    "            label[idx_part] = 1\n",
    "\n",
    "        return tiles, label\n",
    "    \n",
    "# --- MODEL --- #\n",
    "class TileMobileNet(nn.Module):\n",
    "    def __init__(self, num_parts=NUM_PARTS):\n",
    "        super().__init__()\n",
    "        backbone = models.mobilenet_v2(pretrained=True)\n",
    "        self.feature_extractor = backbone.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_parts),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape  # B=batch, T=tiles\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.feature_extractor(x)\n",
    "        pooled = self.pool(feats).view(B, T, -1)\n",
    "        aggregated = pooled.mean(dim=1)\n",
    "        out = self.classifier(aggregated)\n",
    "        return out\n",
    "\n",
    "# --- TRAINING & EVAL --- #\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_time, total_pixels = 0, 0, 0\n",
    "    global gpu_memories, cpu_memories\n",
    "    gpu_memories, cpu_memories = [], []\n",
    "\n",
    "    for tiles, labels in tqdm(dataloader):\n",
    "        tiles = tiles.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        preds = model(tiles)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time = time.time() - start_time\n",
    "        total_loss += loss.item()\n",
    "        total_time += batch_time\n",
    "        total_pixels += tiles.numel()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_mem_used = mem_info.used / (1024**2)\n",
    "            gpu_memories.append(gpu_mem_used)\n",
    "        else:\n",
    "            gpu_mem_used = 0\n",
    "        \n",
    "        cpu_mem_used = psutil.virtual_memory().used / (1024**2)\n",
    "        cpu_memories.append(cpu_mem_used)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, Time: {total_time:.2f}s, Pixels: {total_pixels}\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tiles, labels in tqdm(dataloader):\n",
    "            tiles = tiles.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            preds = model(tiles)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds_bin = (torch.sigmoid(preds) > 0.5).cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "\n",
    "            all_preds.append(preds_bin)\n",
    "            all_labels.append(labels_np)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    Y_pred = np.vstack(all_preds)\n",
    "    Y_true = np.vstack(all_labels)\n",
    "\n",
    "    micro_f1 = f1_score(Y_true, Y_pred, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(Y_true, Y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    FN = np.logical_and(Y_true == 1, Y_pred == 0).sum()\n",
    "    TP = np.logical_and(Y_true == 1, Y_pred == 1).sum()\n",
    "    FP = np.logical_and(Y_true == 0, Y_pred == 1).sum()\n",
    "\n",
    "    N_images = Y_true.shape[0]\n",
    "    miss_rate = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    fppi = FP / N_images\n",
    "\n",
    "    overall_acc = accuracy_score(Y_true.flatten(), Y_pred.flatten())\n",
    "    overall_prec = precision_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_rec = recall_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "    overall_f1 = f1_score(Y_true.flatten(), Y_pred.flatten(), zero_division=0)\n",
    "\n",
    "    print(f\"Micro F1: {micro_f1:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Miss Rate: {miss_rate:.4f}, FPPI: {fppi:.4f}\")\n",
    "    print(f\"Overall Acc: {overall_acc:.4f}, Precision: {overall_prec:.4f}, Recall: {overall_rec:.4f}, F1: {overall_f1:.4f}\")\n",
    "\n",
    "# --- MAIN SCRIPT --- #\n",
    "if __name__ == \"__main__\":\n",
    "    # Load annotations\n",
    "    json_path = \"../data/processed/final_annotations_without_occluded.json\"\n",
    "    image_dir = \"../data/images\"\n",
    "    with open(json_path) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Split dataset keys\n",
    "    image_ids = list(annotations[\"images\"].keys())\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    n = len(image_ids)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_test = n - n_train\n",
    "    n_val = int(0.1 * n_train)\n",
    "    n_train = n_train - n_val  # adjust train count after val split\n",
    "\n",
    "    train_ids = image_ids[:n_train]\n",
    "    val_ids = image_ids[n_train:n_train + n_val]\n",
    "    test_ids = image_ids[n_train + n_val:]\n",
    "\n",
    "    print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
    "\n",
    "    # Extract training subset for anchor computations\n",
    "    train_images_subset = {k: annotations[\"images\"][k] for k in train_ids}\n",
    "    all_parts = annotations[\"all_parts\"]\n",
    "\n",
    "    # Find anchor part based on training set\n",
    "    anchor_part = find_anchor_part(train_images_subset, all_parts, image_dir)\n",
    "    avg_anchor_pos, average_offsets = load_avg_positions_from_subset_with_anchor(train_images_subset, all_parts, anchor_part, image_dir)\n",
    "\n",
    "    print(f\"Anchor part: {anchor_part}\")\n",
    "    print(f\"Average anchor position: {avg_anchor_pos}\")\n",
    "    print(f\"Average offsets for parts (example): {dict(list(average_offsets.items())[:3])}\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = BikeTileDataset(annotations, image_dir, train_ids, all_parts, average_offsets)\n",
    "    val_dataset = BikeTileDataset(annotations, image_dir, val_ids, all_parts, average_offsets)\n",
    "    test_dataset = BikeTileDataset(annotations, image_dir, test_ids, all_parts, average_offsets)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = TileMobileNet().to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        train(model, train_loader, optimizer, criterion)\n",
    "        evaluate(model, val_loader, criterion)\n",
    "        max_gpu_mem = max(gpu_memories) if gpu_memories else 0\n",
    "        max_cpu_mem = max(cpu_memories)\n",
    "\n",
    "        table = [\n",
    "            [\"Epoch\", epoch],\n",
    "            [\"Maximum GPU Memory Usage (MB)\", f\"{max_gpu_mem:.2f}\"],\n",
    "            [\"Maximum CPU Memory Usage (MB)\", f\"{max_cpu_mem:.2f}\"],\n",
    "        ]\n",
    "\n",
    "        print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "evaluate(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
